\documentclass{article}

\begin{document}

\title{Stochastic Optimization and Automatic Differentiation for Machine Learning}
\author{Guillaume Demonet}

\maketitle

\begin{abstract}
We consider in this project a method for controlling variance of gradient-based
estimators, using control variates as the well-known algorithms SVRG or SAGA.
In the considered article, the control variates are designed to track these
gradients through an approximation of the Hessian.
\end{abstract}

\section*{Introduction}

Hello world - that's great ain't it? I love it

\section{Control variates methods}

<TODO>

\section{Hessian matrix for better tracking}

<TODO>

\section{Implementation}

<TODO>

% Ideas:
%
% - generic context for updating parameters (+ tensorflow approach)
% - control variates framework : definition, benchmark and evaluation
% - implementation of SVRG in this framework
% - implementation of the Hessian-based versions (3, from the article)
% - optimized algorithms (parallelism, JIT compilation)
% - multiple control graphs to output (time, space, accuracy, correlation of
%   the control variates, etc.)

\section{Results}

<TODO>

% Ideas:
%
% - simple least squares loss as in the lesson's notebooks
% - apply to softmax and others
% - use some simple dataset to test the pure Python implementation
% - include Tensorflow implementation
% - benchmark the TF version using real-scale NN on CIFAR-100

\section*{Conclusion}

Well, that was refreshing!

\end{document}